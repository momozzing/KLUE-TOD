from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained('skt/ko-gpt-trinity-1.2B-v0.5')

test_list = [565, 404, 459, 32771, 439, 461, 406, 22509, 20801, 35675, 42852, 30390, 33521, 23140, 18932, 31194, 32751, 38143, 21753, 32141, 22068, 388, 30003, 31438, 45145, 33176, 404, 445, 32771, 439, 461, 406]
test_list2 = [51200, 30189, 20801, 35675, 42852, 30390, 33521, 23140, 18932, 31194, 32751, 38143, 21753, 32141, 22068, 388, 30003, 31438, 45145, 33176, 51204]
test_list3 = [51200, 30189, 20801, 35675, 42852, 30390, 33521, 23140, 18932, 31194, 32751, 38143, 21753, 32141, 22068, 388, 30003, 31438, 45145, 33176, 51204, 51201, 30989, 30189, 20801, 35675, 42852, 32751, 47056, 42500, 43563, 30101, 24644, 30091, 30599, 30241, 32141, 42160, 30960, 30036, 25656, 30461, 30003, 34437, 30413, 44084, 34620, 395, 35347, 30599, 51205, 51200, 30241, 40109, 30413, 30059, 33217, 30003, 33599, 42968, 30002, 24645, 33184, 30735, 50400, 31166, 31579, 44485, 30197, 51204]
test_list4 = [51202, 39990, 389, 31353, 389, 34651, 38970, 24645, 20577, 389, 26093, 22208, 389, 25828, 24645, 20577, 24645, 20577, 389, 26212, 22208, 31022, 389, 465, 30541, 24645, 20577, 389, 38513, 20584, 389, 444, 36270, 443, 38909, 51202]
test_list5 = [51200, 30189, 20801, 35675, 42852, 30390, 33521, 23140, 18932, 31194, 32751, 38143, 21753, 32141, 22068, 388, 30003, 31438, 45145, 33176, 51204, 51201, 30989, 30189, 20801, 35675, 42852, 32751, 47056, 42500, 43563, 30101, 24644, 30091, 30599, 30241, 32141, 42160, 30960, 30036, 25656, 30461, 30003, 34437, 30413, 44084, 34620, 395, 35347, 30599, 51205, 51200, 30241, 40109, 30413, 30059, 33217, 30003, 33599, 42968, 30002, 24645, 33184, 30735, 50400, 31166, 31579, 44485, 30197, 51204, 51201, 31859, 34410, 37016, 25429, 39548, 30029, 40683, 31166, 30761, 33028, 24645, 30358, 32796, 31214, 51205, 51200, 31859, 29979, 39990, 29980, 45293, 31446, 24644, 30714, 32318, 30139, 22509, 38064, 31800, 33176, 51204]
SPECIAL_TOKENS = ['<sos_u>', '<sos_r>', '<sos_b>', '<sos_a>', '<eos_u>', '<eos_r>', '<eos_b>', 
            '<eos_a>', '<sos_context>', '<eos_context>']

tokenizer.add_tokens(SPECIAL_TOKENS)

print(tokenizer.convert_ids_to_tokens(test_list2))
print(tokenizer.convert_ids_to_tokens(test_list3))
print(tokenizer.convert_ids_to_tokens(test_list4))
print(tokenizer.convert_ids_to_tokens(test_list5))