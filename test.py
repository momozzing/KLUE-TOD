from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained('skt/ko-gpt-trinity-1.2B-v0.5')

test_list = [51200, 30165, 30526, 31182, 25596, 30413, 30063, 33217, 35598, 23468,
        25292, 23468, 38064, 41254, 34636, 51204, 51201, 35853, 33176, 34477,
        39922, 31166, 31193, 30348, 24648, 49061, 51205, 51200, 29987, 30111,
        25596, 33560, 30020, 30501, 31562, 25820, 31181, 21752, 30476, 35891,
        25372, 32595, 44019, 20780, 30314, 30258, 35891, 36568, 51204, 51201,
        30413, 31133, 30079, 30095, 36159, 35598, 23468, 25292, 23468, 31508,
        32399, 34115, 30012, 28348, 25768, 33372, 45584, 51205, 51200, 29987,
        20108, 30012, 28348, 29980, 31342, 23160, 49892, 51204, 51201, 34874,
        28936, 33344, 31075, 30191, 25976, 30106, 32106, 29980, 31316, 32837,
        30820, 30940, 30465, 34934, 51205, 51200, 31859, 30223, 32665, 33563,
        18796, 30169, 22509, 38064, 30006, 30499, 32174, 51204, 51201, 30183,
        38064, 30314, 35772, 38064, 32974, 42415, 34636, 51205, 51200, 30183,
        30569, 32667, 33176, 38064, 41883, 21753, 33019, 35463, 25429, 20780,
        38220, 32892, 45145, 33649, 51204, 51201, 38064, 30306, 34285, 30782,
        31785,   411,   399, 30599, 33019, 35463, 40950, 30625, 20584, 25837,
        34429, 30599, 31255, 30105, 30071, 25632, 30816, 30464, 34636, 51205,
        51200, 30020, 32174, 36764, 30003, 36333, 32124, 35772, 51204, 51209,
        51202, 51202, 51201, 30183, 32124, 31214, 51205,     3]
test_list2 = [51201, 30183, 32124, 31214, 51205,     1,     3,     3]
test_list3 = [51200, 30189, 20801, 35675, 42852, 30390, 33521, 23140, 18932, 31194, 32751, 38143, 21753, 32141, 22068, 388, 30003, 31438, 45145, 33176, 51204, 51201, 30989, 30189, 20801, 35675, 42852, 32751, 47056, 42500, 43563, 30101, 24644, 30091, 30599, 30241, 32141, 42160, 30960, 30036, 25656, 30461, 30003, 34437, 30413, 44084, 34620, 395, 35347, 30599, 51205, 51200, 30241, 40109, 30413, 30059, 33217, 30003, 33599, 42968, 30002, 24645, 33184, 30735, 50400, 31166, 31579, 44485, 30197, 51204]
test_list4 = [51202, 39990, 389, 31353, 389, 34651, 38970, 24645, 20577, 389, 26093, 22208, 389, 25828, 24645, 20577, 24645, 20577, 389, 26212, 22208, 31022, 389, 465, 30541, 24645, 20577, 389, 38513, 20584, 389, 444, 36270, 443, 38909, 51202]
test_list5 = [51200, 30189, 20801, 35675, 42852, 30390, 33521, 23140, 18932, 31194, 32751, 38143, 21753, 32141, 22068, 388, 30003, 31438, 45145, 33176, 51204, 51201, 30989, 30189, 20801, 35675, 42852, 32751, 47056, 42500, 43563, 30101, 24644, 30091, 30599, 30241, 32141, 42160, 30960, 30036, 25656, 30461, 30003, 34437, 30413, 44084, 34620, 395, 35347, 30599, 51205, 51200, 30241, 40109, 30413, 30059, 33217, 30003, 33599, 42968, 30002, 24645, 33184, 30735, 50400, 31166, 31579, 44485, 30197, 51204, 51201, 31859, 34410, 37016, 25429, 39548, 30029, 40683, 31166, 30761, 33028, 24645, 30358, 32796, 31214, 51205, 51200, 31859, 29979, 39990, 29980, 45293, 31446, 24644, 30714, 32318, 30139, 22509, 38064, 31800, 33176, 51204]
SPECIAL_TOKENS = ['<sos_u>', '<sos_r>', '<sos_b>', '<sos_a>', '<eos_u>', '<eos_r>', '<eos_b>', 
            '<eos_a>', '<sos_context>', '<eos_context>']


# test_list = [2, 10070, 49801,  9105,  7317,  8006,   406]
tokenizer.add_tokens(SPECIAL_TOKENS)

# print(tokenizer.convert_ids_to_tokens(test_list2))
# print(tokenizer.convert_ids_to_tokens(test_list3))
# print(tokenizer.convert_ids_to_tokens(test_list4))
# print(tokenizer.convert_ids_to_tokens(test_list5))


print(tokenizer.convert_ids_to_tokens(test_list))
print(tokenizer.convert_ids_to_tokens(test_list2))

# print(tokenizer.bos_token_id, tokenizer.eos_token_id)

